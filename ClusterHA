#!/bin/bash

# Tips: DO THIS ON ALL NODES!

# sudo -i
# rm /root/cluster
# nano /root/cluster
# <-CTRL A-> <-CTRL C->
# <-CTRL O-> <-CTRL X->
# chmod +x /root/cluster
# reboot

# sudo crontab -e:
# "@reboot bash /root/cluster"
# reboot

# Remember to check if there are .contok and .vmok  files!
# sudo touch /srv/containers/.contok
# sudo touch /srv/virtualmachines/.vmok

function init {

 ##############################################################################
# ============================================================================ #

                                # =========== #
                               # Control Panel #
                                # =========== #

# Local Gateway
  export IP3="172.20.0.1"

# LAN 01
  export IP1="172.20.0.10"

# LAN 02
  export IP2="172.20.0.11"

                              # ================= #
                             # Advanced Parameters #
                              # ================= #

# Reserved NIC 01
  export HOSTNAME1="cluster-01"

# Reserved NIC 02
  export HOSTNAME2="cluster-02"

# If not setting this, will run as root...
  export USERNAME="administrador"

# All logs that are saved or discarded goes here
  export logpath="/var/log/gluster"

# Will run 7 times if 0 and 6!
  export beeplow="0"
  export beephigh="6"

  export loglow="0"
  export loghigh="6"

# All texts and logs!
  export TEXT01="WARNING: Glusterd started manually"
  export TEXT02="Proceeding anyway."
  export TEXT03="ERROR: Glusterd not running!"
  export TEXT04="ERROR: Shutting down everything!"
  export TEXT05="ERROR: Reserved NIC not responding!"
  export TEXT06="ERROR: Gateway not found"
  export TEXT07="ERROR: The other node doesn't respond."
  export TEXT08="ERROR: Starting everything myself!"
  export TEXT09="ERROR: Containers volume was not found!"
  export TEXT10="Or maybe I just can't found the ctrl file. Check it out!"
  export TEXT11="ERROR: Shutting down everything to prevent Split Brain!"
  export TEXT12="ERROR: Virtual Machines volume was not found!"
  export TEXT13="Status: Everything is STOPPED on this node!"
  export TEXT14="Status: Everything is RUNNING on this node!"
  export TEXT15="Silence! Or I will aniquilate you!"

# ============================================================================ #
 ##############################################################################



 ##############################################################################
# ============================================================================ #

             # ======================================= #
            # Scripts to pre adjusting and controlling! #
             # ======================================= #

# Resetting beep on first run!
  export beepsound="0"

# Check if everything if Ok with SSH to connect to another node
  eval $(ssh-agent) >/dev/null
  ssh-add /home/$USERNAME/.ssh/id_rsa >/dev/null

# This will keep this script running like a daemon!
  while true; do
    export data=$(date +"%H:%M %d.%m.%Y")
    sleep 5
    cluster00
  done

                   # ========================= #
                  # High Availability Functions #
                   # ========================= #

}

# 00 - Check if GlusterFS service is Running
function cluster00 {
  systemctl is-active --quiet glusterd && {
    cluster01
  } || {
    systemctl start glusterd.service
    systemctl is-active --quiet glusterd && {
    textlog $TEXT01
    textlog $TEXT02
      cluster01
    } || {
      textlog $TEXT03
      textlog $TEXT04
      beeps "2000" "2000"
      setpid "0" >/dev/null
      cluster08
    }
  }
}

# 01 - Ping the other node through reserved NIC
function cluster01 {
  if ping -c 1 "$HOSTNAME1" >/dev/null && ping -c 1 "$HOSTNAME2" >/dev/null; then
    cluster02
  else
    textlog $TEXT05
    textlog $TEXT04
    beeps "1000" "2000"
    setpid "0" >/dev/null
    cluster08
  fi
}

# 02 - Ping Gateway
function cluster02 {
  if ping -c 1 $IP3 >/dev/null; then
    cluster03
  else
    textlog $TEXT06
    textlog $TEXT04
    beeps "2000" "1000"
    setpid "0" >/dev/null
    cluster08
  fi
}

# 03 - Ping the other node
function cluster03 {
  if ping -c 1 $IP1 >/dev/null && ping -c 1 $IP2 > /dev/null; then
    cluster04
  else
    textlog $TEXT07
    textlog $TEXT08
    setpid "1" >/dev/null
    cluster08
  fi
}

# 04 - Checking for .lockha file on the other node
function cluster04 {
  [ "$HOSTNAME" = "$HOSTNAME1" ] && {
    checking=$(ssh $USERNAME@$HOSTNAME2 "cat /home/$USERNAME/.lockha") &&
    cluster05
  } || {
    checking=$(ssh $USERNAME@$HOSTNAME1 "cat /home/$USERNAME/.lockha") &&
    cluster05
  }
}

# 05 - Check if there are /srv/containers volume!
function cluster05 {
  [ -f "/srv/containers/.contok" ] && {
    cluster06
  } || {
    textlog $TEXT09
    textlog $TEXT10
    textlog $TEXT11
    beeps "1000" "1000"
    setpid "0" >/dev/null
    cluster08
  }
}

# 06 - Check if there are /srv/virtualmachines volume!
function cluster06 {
  [ -f "/srv/virtualmachines/.vmok" ] && {
    cluster08
  } || {
    textlog $TEXT12
    textlog $TEXT10
    textlog $TEXT11
    beeps "1000" "1000"
    setpid "0" >/dev/null
    cluster08
  }
}

# 07 - Starting or stopping all containers and VMs accordingly
# If stopping, will wait 20s!
function cluster08 {
  export beepsound="0"

  [ "$checking" = "1" ] && {
    setpid "0" >/dev/null
    dockerctrl stop && >/dev/null
    vmctrl "--state-running" "shutdown" && >/dev/null
    textlog $TEXT13
  } || {
    setpid "1" >/dev/null
    sleep 20
    dockerctrl start && >/dev/null
    vmctrl "--state-shutoff" "start" && >/dev/null
    textlog $TEXT14
  }
}

                            # =============== #
                           # Control Functions #
                            # =============== #

# This starts or stops all containers!
function dockerctrl {
  docker "$1" $(docker ps -a -q)
}

# This starts or stops all virtual machines!
function vmctrl {
  for i in $(virsh list --all "$1" | awk '{print $2}' | grep -v Nome); do
    virsh "$2" "$i"
  done
}

# Setting PID control file on "/home/$USERNAME/.lockha".
# This randomly ensures that we don't fall on a ping-pong situation,
# with the machine nodes getting the same number for .lockha file!
function setpid {
  sleep $((RANDOM%5))
  echo "$1" > /home/"$USERNAME"/.lockha
}

# Just some logging
function textlog {
  echo "$data $1" | tee -a $logpath

  sizelog=$(du --apparent-size --block-size=1 $logpath | awk {print'$1'})
  [ "$sizelog" -ge "50000000" ] && {
    logmngr
  } || {
    echo "." > /dev/null
  }
}

# All beep configurations are here.
function beeps {
  [ "$beepsound" -ge "$beeplow" ] && [ "$beepsound" -le "$beephigh" ] && {
    modprobe pcspkr
    env -u SUDO_GID -u SUDO_COMMAND -u SUDO_USER -u SUDO_UID beep -f "$1" -l 200 -r 1
    env -u SUDO_GID -u SUDO_COMMAND -u SUDO_USER -u SUDO_UID beep -f "$2" -l 200 -r 1
    export beepsound=$(($beepsound+1))
  } || {
    textlog $TEXT14
  }
}

function logmgr {
  rm -rf /home/$USERNAME/.gluster_log
  rm -rf $logpath

  touch $logpath
  ln -s $logpath /home/$USERNAME/.gluster_log
}

# ============================================================================ #
 ##############################################################################

                                  #######
                                 # Start #
                                  #######

# Prepare the Logs
logmgr

# Let's get Started!
init

exit 1
