#!/bin/bash

# Tips: DO ON ALL NODES!

# sudo -i
# rm /root/cluster
# nano /root/cluster
# -CTRL A- -CTRL C-
# chmod +x /root/cluster
# reboot

# sudo crontab -e:
# "@reboot bash /root/cluster"
# reboot

# Remember to check if there are .contok and/or .vmok  files!
# sudo touch /srv/containers/.contok
# sudo touch /srv/virtualmachines/.vmok

function init {

 ##############################################################################
# ============================================================================ #

                                # =========== #
                               # Control Panel #
                                # =========== #

# Local Gateway
  export IP3="172.20.0.1"

# LAN 01
  export IP1="172.20.0.10"

# LAN 02
  export IP2="172.20.0.11"

 # ================= #
# Advanced Parameters #
 # ================= #

# Reserved NIC 01
  export HOSTNAME1="cluster-01"

# Reserved NIC 02
  export HOSTNAME2="cluster-02"

# If not setting this, will run as root...
  export USERNAME="administrador"

# All logs that are saved or discarded goes here
  export logpath="/var/log/gluster"

# Will run 7 times if 0 and 6!
  export beeplow="0"
  export beephigh="6"

  export loglow="0"
  export loghigh="6"

# ============================================================================ #
 ##############################################################################



 ##############################################################################
# ============================================================================ #

             # ======================================= #
            # Scripts to pre adjusting and controlling! #
             # ======================================= #

# Removing old Log files
  rm -rf $logpath
  rm -rf /home/$USERNAME/.gluster_log

# Creating new Log files
  touch $logpath
  ln -s $logpath /home/$USERNAME/.gluster_log

# Resetting beep on first run!
  export beepsound="0"

# Check if everything if Ok with SSH to connect to another node
  eval $(ssh-agent) >/dev/null
  ssh-add /home/$USERNAME/.ssh/id_rsa >/dev/null

# This will keep this script running like a daemon!
  while true; do
    export data=$(date +"%H:%M %d.%m.%Y")
    sleep 5
    cluster00
  done

                   # ========================= #
                  # High Availability Functions #
                   # ========================= #

}

# 00 - Check if GlusterFS service is Running
function cluster00 {
  systemctl is-active --quiet glusterd && {
    cluster01
  } || {
    systemctl start glusterd.service
    systemctl is-active --quiet glusterd && {
    textlog "Glusterd started manually, it was stopped!"
    textlog "Proceeding anyway."
      cluster01
    } || {
      textlog "ERROR: Glusterd not running!"
      textlog "ERROR: Shutting down all containers!"
      beeps "2000" "2000"
      dockerctrl stop && >/dev/null
      vmctrl "--state-running" "shutdown" && >/dev/null
      setpid "0" >/dev/null
    }
  }
}

# 01 - Ping the other node through reserved NIC
function cluster01 {
  if ping -c 1 "$HOSTNAME1" >/dev/null && ping -c 1 "$HOSTNAME2" >/dev/null; then
    cluster02
  else
    textlog "ERROR: Can't connect to Reserved NIC!"
    textlog "ERROR: Shutting down all containers and virtual machines!"
    beeps "1000" "2000"
    dockerctrl stop && >/dev/null
    vmctrl "--state-running" "shutdown" && >/dev/null
    setpid "0" >/dev/null
  fi
}

# 02 - Ping Gateway
function cluster02 {
  if ping -c 1 $IP3 >/dev/null; then
    cluster03
  else
    textlog "ERROR: Gateway not found"
    textlog "ERROR: Shutting down all containers and virtual machines!"
    beeps "2000" "1000"
    dockerctrl stop && >/dev/null
    vmctrl "--state-running" "shutdown" && >/dev/null
    setpid "0" >/dev/null
  fi
}

# 03 - Ping the other node
function cluster03 {
  if ping -c 1 $IP1 >/dev/null && ping -c 1 $IP2 > /dev/null; then
    cluster04
  else
    textlog "ERROR: The other node doesn't respond."
    textlog "ERROR: Starting containers and virtual machines myself!"
    dockerctrl start && >/dev/null
    vmctrl "--state-shutoff" "start" && >/dev/null
    setpid "1" >/dev/null
  fi
}

# 04 - Checking for .lockha file on the other node
function cluster04 {
  [ "$HOSTNAME" = "$HOSTNAME1" ] && {
    checking=$(ssh $USERNAME@$HOSTNAME2 "cat /home/$USERNAME/.lockha") &&
    cluster05
  } || {
    checking=$(ssh $USERNAME@$HOSTNAME1 "cat /home/$USERNAME/.lockha") &&
    cluster05
  }
}

# 05 - Check if there are /srv/containers volume!
function cluster05 {
  [ -f "/srv/containers/.contok" ] && {
    cluster06
  } || {
    textlog "ERROR: Containers volume was not found!"
    beeps "1000" "1000"
    dockerctrl stop && >/dev/null
    setpid "0" >/dev/null
  }
}

# 06 - Check if there are /srv/virtualmachines volume!
function cluster06 {
  [ -f "/srv/virtualmachines/.vmok" ] && {
    cluster08
  } || {
    textlog "ERROR: Virtual Machine volume was not found!"
    beeps "1000" "1000"
    vmctrl "--state-running" "shutdown" && >/dev/null
    setpid "0" >/dev/null
  }
}

# 07 - Starting or stopping all containers and VMs accordingly
# If stopping, will wait 20s!
function cluster08 {
  export beepsound="0"

  [ "$checking" = "1" ] && {
    setpid "0" >/dev/null
    dockerctrl stop && >/dev/null
    vmctrl "--state-running" "shutdown" && >/dev/null
    textlog "Everything is Ok!"
    textlog "All containers and Virtual Machines are stopped on this node!"
  } || {
    setpid "1" >/dev/null
    sleep 20
    dockerctrl start && >/dev/null
    vmctrl "--state-shutoff" "start" && >/dev/null
    textlog "Everything is Ok!"
    textlog "All containers and Virtual Machines are running on this node!"
  }
}

                            # =============== #
                           # Control Functions #
                            # =============== #

# Will wait to all containers to stop or start before continuing!
# Extra Security check: If "/srv/containers" doesn't exist after all checks, 
# stop all containers!
function dockerctrl {
  [ -f /srv/containers/.contok ] && {
    docker "$1" $(docker ps -a -q)
  } || {
    textlog "ERROR: Containers volume was not found after all checks!"
    textlog "ERROR: Shutting down all containers that are still alive!"
    docker stop $(docker ps -a -q)
  }
}

# Will wait all VMs to stop or start before continuing!
# Extra Security check: If "/srv/virtualmachines" doesn't exist after all checks, 
# stop all virtual machines!
function vmctrl {
  [ -f /srv/virtualmachines/.vmok ] && {
    for i in $(virsh list --all "$1" | awk '{print $2}' | grep -v Nome); do
      virsh "$2" "$i"
    done
  } || {
    textlog "Virtual Machine volume was not found after all checks!"
    textlog "ERROR: Shutting down all virtual machines that are still alive!"
    vmctrl "--state-running" "shutdown" && >/dev/null
  }
}

# All beep configurations are here.
function beeps {
  [ "$beepsound" -ge "$beeplow" ] && [ "$beepsound" -le "$beephigh" ] && {
    modprobe pcspkr
    env -u SUDO_GID -u SUDO_COMMAND -u SUDO_USER -u SUDO_UID beep -f "$1" -l 200 -r 1
    env -u SUDO_GID -u SUDO_COMMAND -u SUDO_USER -u SUDO_UID beep -f "$2" -l 200 -r 1
    export beepsound=$(($beepsound+1))
  } || {
    echo "Silence! Or I kill you!" >/dev/null
  }
}

# Setting PID control file on "/home/$USERNAME/.lockha".
# This randomly ensures that we don't fall on a ping-pong situation,
# with the machine nodes getting the same number for .lockha file!
function setpid {
  sleep $((RANDOM%5))
  echo "$1" > /home/"$USERNAME"/.lockha
}

# Just some logging
function textlog {
  echo "$data $1" | tee -a $logpath
}

# ============================================================================ #
 ##############################################################################

# Let's Get Started!
init

exit 1
